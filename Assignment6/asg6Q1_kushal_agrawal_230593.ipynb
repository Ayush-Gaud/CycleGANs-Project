{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFK-MaPSg3zt"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.datasets.mnist import load_data\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Concatenate\n",
        "import numpy as np\n",
        "from numpy import expand_dims\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPIFApOehEJc"
      },
      "outputs": [],
      "source": [
        "(trainX, trainy), (testX, testy) = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cY9VZsehEMf"
      },
      "outputs": [],
      "source": [
        "# function to define discriminator\n",
        "def define_discriminator(in_shape=(28,28,1), n_classes=10):\n",
        "\t# Input for class labels\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# Embedding for label input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# Scale up to image dimensions with linear activation\n",
        "\tn_nodes = in_shape[0] * in_shape[1]\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# Reshape to additional channel\n",
        "\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\t# Input for image\n",
        "\tin_image = Input(shape=in_shape)\n",
        "\t# Concatenate image and label embeddings\n",
        "\tmerge = Concatenate()([in_image, li])\n",
        "\t# Convolutional layers with leaky relu\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\tfe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# Flatten feature maps\n",
        "\tfe = Flatten()(fe)\n",
        "\t# Dropout layer for regularization\n",
        "\tfe = Dropout(0.4)(fe)\n",
        "\t# Output layer with sigmoid activation\n",
        "\tout_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\t# Model\n",
        "\tmodel = Model([in_image, in_label], out_layer)\n",
        "\t# Compile model with binary cross-entropy loss and Adam optimizer\n",
        "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\treturn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01r6kcHohEPb"
      },
      "outputs": [],
      "source": [
        "def define_generator(latent_dim, n_classes=10):\n",
        "\t# Input for class labels\n",
        "\tin_label = Input(shape=(1,))\n",
        "\t# Embedding for label input\n",
        "\tli = Embedding(n_classes, 50)(in_label)\n",
        "\t# Scale up to image dimensions with linear activation\n",
        "\tn_nodes = 7 * 7\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\t# Reshape to additional channel\n",
        "\tli = Reshape((7, 7, 1))(li)\n",
        "\t# Input for noise\n",
        "\tin_lat = Input(shape=(latent_dim,))\n",
        "\t# Number of nodes\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\t# Linear activation\n",
        "\tgen = Dense(n_nodes)(in_lat)\n",
        "\t# Apply leaky relu\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# Reshape to additional channel\n",
        "\tgen = Reshape((7, 7, 128))(gen)\n",
        "\t# Merge image gen and label input\n",
        "\tmerge = Concatenate()([gen, li])\n",
        "\t# Upsample\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "\t# Apply leaky relu\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# Upsample\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\t# Apply leaky relu\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\t# Output\n",
        "\tout_layer = Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n",
        "\t# Model\n",
        "\tmodel = Model([in_lat, in_label], out_layer)\n",
        "\treturn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcgTZE18hESd"
      },
      "outputs": [],
      "source": [
        "def define_gan(g_model, d_model):\n",
        "\t# Make discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# Input for generator (noise and labels)\n",
        "\tgen_noise, gen_label = g_model.input\n",
        "\t# Output from generator\n",
        "\tgen_output = g_model.output\n",
        "\t# Output from discriminator given generated images and labels\n",
        "\tgan_output = d_model([gen_output, gen_label])\n",
        "\t# Define GAN model with generator inputs and discriminator output\n",
        "\tmodel = Model([gen_noise, gen_label], gan_output)\n",
        "\t# Compile GAN with Adam optimizer and binary cross-entropy loss\n",
        "\topt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\treturn model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkruMc0dhEVZ"
      },
      "outputs": [],
      "source": [
        "def load_real_samples():\n",
        "    # Load MNIST dataset\n",
        "    (trainX, trainy), (testX, testy) = load_data()\n",
        "    # Expand dimensions of trainX to include channel dimension\n",
        "    X = expand_dims(trainX, axis=-1)\n",
        "    # Convert to float32\n",
        "    X = X.astype('float32')\n",
        "    # Normalize to range [-1, 1]\n",
        "    X = (X - 127.5) / 127.5\n",
        "    # Return normalized images and corresponding labels\n",
        "    return [X, trainy]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQ3YevAohEYT"
      },
      "outputs": [],
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        "    # Unpack dataset into images and labels\n",
        "    images, labels = dataset\n",
        "    # Randomly select indices\n",
        "    ix = randint(0, images.shape[0], n_samples)\n",
        "    # Select images and corresponding labels based on indices\n",
        "    X, labels = images[ix], labels[ix]\n",
        "    # Generate 'real' class labels (1 for real)\n",
        "    y = ones((n_samples, 1))\n",
        "    # Return selected images and labels with 'real' labels\n",
        "    return [X, labels], y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KINkMdtEhEbX"
      },
      "outputs": [],
      "source": [
        "def generate_latent_points(latent_dim, n_samples, n_classes=10):\n",
        "    # Generate random input for latent space\n",
        "    x_input = randn(latent_dim * n_samples)\n",
        "    # Reshape into a batch of inputs for the generator\n",
        "    z_input = x_input.reshape(n_samples, latent_dim)\n",
        "    # Generate random labels\n",
        "    labels = randint(0, n_classes, n_samples)\n",
        "    # Return generated latent points and labels\n",
        "    return [z_input, labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuD1MqIKhEep"
      },
      "outputs": [],
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    # Generate latent points and random labels\n",
        "    z_input, labels_input = generate_latent_points(latent_dim, n_samples)\n",
        "    # Generate fake images using the generator\n",
        "    images = generator.predict([z_input, labels_input])\n",
        "    # Generate 'fake' class labels (0 for fake)\n",
        "    y = zeros((n_samples, 1))\n",
        "    # Return generated fake images with corresponding labels\n",
        "    return [np.array(images), labels_input], y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rF9PZnDhEhv"
      },
      "outputs": [],
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
        "    # Calculate number of batches per epoch\n",
        "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "    # Half batch size for each real and fake samples\n",
        "    half_batch = int(n_batch / 2)\n",
        "\n",
        "    # Iterate over epochs\n",
        "    for i in range(n_epochs):\n",
        "        # Iterate over batches\n",
        "        for j in range(bat_per_epo):\n",
        "            # Train discriminator on real samples\n",
        "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "            d_loss_real, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "\n",
        "            # Train discriminator on fake samples\n",
        "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            d_loss_fake, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "\n",
        "            # Train generator via GAN model\n",
        "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = ones((n_batch, 1))\n",
        "            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "\n",
        "            # Print progress\n",
        "            print('Epoch>%d, Batch%d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "                  (i+1, j+1, bat_per_epo, d_loss_real, d_loss_fake, g_loss))\n",
        "\n",
        "    # Save the generator model after training\n",
        "    g_model.save('mnist_conditional_generator.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fziJly0WhEk6",
        "outputId": "a5a0b84f-5e2d-4a1e-93c2-11317cb3c7d3"
      },
      "outputs": [],
      "source": [
        "\n",
        "latent_dim = 100\n",
        "d_model = define_discriminator()\n",
        "g_model = define_generator(latent_dim)\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "dataset = load_real_samples()\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100,n_batch=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8R0H52FshEn2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from numpy import asarray\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from keras.models import load_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XuPwQteThEra"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = load_model('mnist_conditional_generator.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8rLSvtIVqYsD"
      },
      "outputs": [],
      "source": [
        "#plotting the results\n",
        "latent_points, labels = generate_latent_points(100, 100)\n",
        "labels = asarray([x for _ in range(10) for x in range(10)])\n",
        "X  = model.predict([latent_points, labels])\n",
        "X = (X + 1) / 2.0\n",
        "X = (X*255).astype(np.uint8)\n",
        "def show_plot(examples, n):\n",
        "\tfor i in range(n * n):\n",
        "\t\tplt.subplot(n, n, 1 + i)\n",
        "\t\tplt.axis('off')\n",
        "\t\tplt.imshow(examples[i, :, :, :],cmap='gray_r')\n",
        "\tplt.show()\n",
        "\n",
        "show_plot(X, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2NizSD15qYun"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JWbQ80F3qYx-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
