{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1567181,"sourceType":"datasetVersion","datasetId":925857}],"dockerImageVersionId":30066,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa                     #importing libraries\nimport glob\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport os\nimport concurrent.futures\nimport random\nimport time\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_A_paths = glob.glob('../input/summer2winter-yosemite/trainA/*.jpg')\ntrain_B_paths = glob.glob('../input/summer2winter-yosemite/trainB/*.jpg')\nprint('length of train_A：',len(train_A_paths))\nprint('length of train_B：',len(train_B_paths))              #importing datsets (training data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_A_paths = glob.glob('../input/summer2winter-yosemite/testA/*.jpg')\ntest_B_paths = glob.glob('../input/summer2winter-yosemite/testB/*.jpg')\nprint('length of test_A：',len(test_A_paths))                  #importing datsets(testing )\nprint('length of test_B：',len(test_B_paths))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMSIZE = 256\n# Function to read an image\ndef read_img(image):\n    img = tf.keras.preprocessing.image.load_img(image, color_mode='rgb', target_size=(IMSIZE, IMSIZE))\n    return img\n# Function to prepare the image dataset\ndef prepare_dataset(namelist):\n    start = time.time()\n    imgs = []\n     # Use ThreadPoolExecutor for parallel image loading (16 threads\n    with concurrent.futures.ThreadPoolExecutor(max_workers = 16) as executor:\n        i = 0\n        for value in executor.map(read_img, namelist):\n            i+=1\n            print(\"\\rFetching: [{}/{}]\".format(i, len(namelist)), end=\"\", flush=True)\n            # Append loaded image to the list\n            imgs.append(value)\n        imgs = np.stack(imgs)\n        # Convert the array to a TensorFlow tensor\n        imgs = tf.convert_to_tensor(imgs)\n    print(\"\\nExecution time: \",time.time() - start, \"s\")\n    return imgs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device('/cpu:0'):\n    train_a= prepare_dataset(train_A_paths)\n    train_b= prepare_dataset(train_B_paths)\n    test_a= prepare_dataset(test_A_paths)\n    test_b= prepare_dataset(test_B_paths)\nprint(\"Training A tensor shape\", train_a.shape)\nprint(\"Training B tensor shape\", train_b.shape)\nprint(\"Testing A tensor shape\", test_a.shape)\nprint(\"Testing B tensor shape\", test_b.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map(image):\n\n    image = tf.cast(image, tf.float32)                   # Preprocesses an image \n    image = image / 255\n    image = image * 2 - 1\n    \n    return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_a = tf.data.Dataset.from_tensor_slices(train_a)\ntrain_b = tf.data.Dataset.from_tensor_slices(train_b)\ntest_a = tf.data.Dataset.from_tensor_slices(test_a)\ntest_b = tf.data.Dataset.from_tensor_slices(test_b)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\nBUFFER_SIZE = 200\n# Preprocess and prepare training and testing datasets\ntrain_a = train_a.map(map, \n                      num_parallel_calls=AUTOTUNE).shuffle(BUFFER_SIZE).batch(1).prefetch(AUTOTUNE)\ntrain_b = train_b.map(map, \n                      num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1).prefetch(AUTOTUNE)\ntest_a = test_a.map(map, \n                      num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1).prefetch(AUTOTUNE)\ntest_b = test_b.map(map, \n                      num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(1).prefetch(AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.zip((train_a, train_b))# Combine training data (train_a and train_b) into a single dataset\ntest_dataset = tf.data.Dataset.zip((test_a, test_b))   # Combine testing data (test_a and test_b) into a single dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 3))                      #visualizing\nfor imgs_A, imgs_B in test_dataset.take(1):\n    plt.subplot(1,2,1)\n    plt.imshow((imgs_A[0]+1)/2)\n    plt.subplot(1,2,2)\n    plt.imshow((imgs_B[0]+1)/2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n\n\ndef downsample(filters, size, apply_batchnorm=True):\n#Creates a downsampling block in the U-Net architecture.\n\n\n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                               use_bias=False))\n\n    if apply_batchnorm:\n        result.add(tfa.layers.InstanceNormalization())\n\n    result.add(tf.keras.layers.LeakyReLU())\n\n    return result\n\n\ndef upsample(filters, size, apply_dropout=False):\n#Creates an upsampling block in the U-Net architecture.\n\n    result = tf.keras.Sequential()\n    result.add(\n        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                        padding='same',\n                                        use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization())\n\n    if apply_dropout:\n        result.add(tf.keras.layers.Dropout(0.5))\n\n    result.add(tf.keras.layers.ReLU())\n\n    return result\n\n# Defines the U-Net generator model architecture.\ndef Generator():\n    inputs = tf.keras.layers.Input(shape=[256,256,3])\n\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n\n    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding='same',\n                                         activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = tf.keras.layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_x = Generator()   # a——>o\ngenerator_y = Generator()   # o——>a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defines the Discriminator model architecture for a PatchGAN.\ndef Discriminator():\n\n\n    inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n\n    down1 = downsample(64, 4, False)(inp) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(\n               512, 4, strides=1,use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization()(conv)\n\n    leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n    last = tf.keras.layers.Conv2D(\n               1, 4, strides=1)(zero_pad2)  # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator_x = Discriminator()   # discriminator  a\ndiscriminator_y = Discriminator()   # discriminator  o\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_input, example_target in test_dataset.take(1):\n    pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_show():\n    \n    title=['Source domain','Target domain','Map']\n    rendered_pictures = generator_x(example_input, training=False)\n    rendered_picture = rendered_pictures[0]\n    fig = plt.figure(figsize=(12, 5))\n    plt.subplot(1,3,1)\n    plt.imshow((example_input[0].numpy()+1)/2)\n    plt.title(title[0])\n    plt.subplot(1,3,2)\n    plt.imshow((example_target[0].numpy()+1)/2)       \n    plt.title(title[1])\n    plt.subplot(1,3,3)\n    plt.imshow((rendered_picture.numpy()+1)/2)\n    plt.title(title[2])\n    plt.show()\n    \n    return rendered_pictures","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = image_show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nLAMBDA = 7\n#Calculates the discriminator loss (combined real and fake loss).\ndef discriminator_loss(disc_real_output, disc_fake_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n    fake_loss = loss_object(tf.zeros_like(disc_fake_output), disc_fake_output)\n    total_disc_loss = real_loss + fake_loss\n    return total_disc_loss\n\n#Calculates the generator loss based on the discriminator's output.\ndef generator_loss(disc_fake_output):\n    gen_loss = loss_object(tf.ones_like(disc_fake_output), disc_fake_output)\n    return gen_loss\n\n#Calculates the cycle consistency loss between a real image and its cycled counterpart.\ndef calc_cycle_loss(real_image, cycled_image):\n    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    return LAMBDA * loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Learning rate for optimizers\ngenerator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_discriminator(image_a, image_b):\n    with tf.GradientTape(persistent=True) as discriminator_tape:\n        #A->B->A\n        fake_b = generator_x(image_a, training=True)\n        cycled_a = generator_y(fake_b, training=True)\n        #B->A->B\n        fake_a = generator_y(image_b, training=True)\n        cycled_b = generator_x(fake_a, training=True)\n        #计算discriminator输出\n        disc_real_a = discriminator_x(image_a, training=True)\n        disc_real_b = discriminator_y(image_b, training=True)\n        disc_fake_a = discriminator_x(fake_a, training=True)\n        disc_fake_b = discriminator_y(fake_b, training=True)\n\n        discriminator_x_loss = discriminator_loss(disc_real_a, disc_fake_a)\n        discriminator_y_loss = discriminator_loss(disc_real_b, disc_fake_b)\n        \n    discriminator_x_gradients = discriminator_tape.gradient(discriminator_x_loss, discriminator_x.trainable_variables)\n    discriminator_y_gradients = discriminator_tape.gradient(discriminator_y_loss, discriminator_y.trainable_variables)\n    \n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,discriminator_x.trainable_variables))\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,discriminator_y.trainable_variables))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\n#  Trains the discriminator models in a CycleGAN architecture.\ndef train_generator(image_a, image_b):\n    with tf.GradientTape(persistent=True) as generator_tape:\n        #A->B->A\n        fake_b = generator_x(image_a, training=True)\n        cycled_a = generator_y(fake_b, training=True)\n        #B->A->B\n        fake_a = generator_y(image_b, training=True)\n        cycled_b = generator_x(fake_a, training=True)\n        disc_fake_a = discriminator_x(fake_a, training=True)\n        disc_fake_b = discriminator_y(fake_b, training=True)\n\n        gen_x_loss = generator_loss(disc_fake_b)\n        gen_y_loss = generator_loss(disc_fake_a)\n        \n        total_cycle_loss = calc_cycle_loss(image_a, cycled_a) + calc_cycle_loss(image_b, cycled_b)\n        \n        total_gen_x_loss = gen_x_loss + total_cycle_loss\n        total_gen_y_loss = gen_y_loss + total_cycle_loss\n        \n    generator_x_gradients = generator_tape.gradient(total_gen_x_loss, generator_x.trainable_variables)\n    generator_y_gradients = generator_tape.gradient(total_gen_y_loss, generator_y.trainable_variables)\n    \n    generator_x_optimizer.apply_gradients(zip(generator_x_gradients, generator_x.trainable_variables))\n    generator_y_optimizer.apply_gradients(zip(generator_y_gradients, generator_y.trainable_variables))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Epochs = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training loop for the CycleGAN.\nfor epoch in range(Epochs):\n    start = time.time()\n    i = 0\n    print ('\\nEpoch {}/{} '.format(epoch+1, Epochs))\n    for img_a, img_b in train_dataset:\n        \n        train_discriminator(img_a, img_b)\n        train_generator(img_a, img_b)\n        \n        percent = float(i+1) * 100 / len(train_dataset)\n        arrow   = '-' * int(percent/100 * 10 - 1) + '>'\n        spaces  = ' ' * (10 - len(arrow))\n        print('\\rTraining: [%s%s] %d %% '% (arrow, spaces, percent), end='', flush=True)\n        i += 1\n    print(\" -\", int(time.time()-start), \"s\", end=\"\")\n    print()\n    \n    if (epoch+1)%5==0:\n        cache = image_show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}